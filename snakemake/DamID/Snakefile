# Dec. 22, 2020
# DamID pipeline
# Functions: 
    # fastqc
    # map, filter (DamID specific), bam_qc
    # call peak (DamID specific): sample level and contrast based
    # bigwig (macs2_DamID signal based)
# Env: 
    # source activate damid
# Requirements
    # inputs in ./fastq/
    # named as {sample}.{R1,R2}.{fastq,fq}.gz
    # e.g. A.R1.fastq.gz A.R2.fastq.gz B...
    # good format of meta.csv and contrast.csv, matching SAMPLES in config.yaml


from snakemake.utils import min_version
min_version("5.17.0")
configfile: "config.yaml"

import pandas as pd
from functools import reduce


SAMPLES=config["SAMPLES"]
GENOME=config["GENOME"]
INDEX=GENOME+".sa"
MQ_MIN=config["MQ_MIN"]
BIN_SIZE=config["BIN_SIZE"]
GSIZE=config["GSIZE"]
SizeFile=config["SizeFile"]

# todo: put this part to damit_module.py (for rule macs2_DamID_contrast)
class parse_meta_contrast_class:
  def __init__(self, y0, y1, y2, y3):
   self.contrast2contrast_name = y0
   self.contrast2treatmentSamples = y1
   self.contrast2controlSamples = y2
   self.contrasts = y3
def parse_meta_contrast (fmeta="meta.csv", fcontrast="contrast.csv"):
    """
    designed to get output dictionaries
    """
    ### parse meta
    meta = pd.read_csv("meta.csv", comment='#')
    #    sample group
    # 0  1-2_S1  ctrl
    # 1  2-1_S2    G1
    # 2  2-2_S3    G1
    # 3  2-3_S4    G1
    # 4  3-1_S5    G2
    # 5  3-2_S6    G2
    # 6  3-3_S7    G2

    ### group2sample
    temp = meta.copy()
    temp['sample'] =  temp.groupby(['group']).transform(lambda x: ','.join(x))
    #                  sample group
    # 0                1-2_S1  ctrl
    # 1  2-1_S2,2-2_S3,2-3_S4    G1
    # 2  2-1_S2,2-2_S3,2-3_S4    G1
    # 3  2-1_S2,2-2_S3,2-3_S4    G1
    # 4  3-1_S5,3-2_S6,3-3_S7    G2
    # 5  3-1_S5,3-2_S6,3-3_S7    G2
    # 6  3-1_S5,3-2_S6,3-3_S7    G2
    temp['sample'] = temp['sample'].apply(lambda x: x.split(','))
    temp.index = temp['group']
    #                              sample group
    # group
    # ctrl                   [1-2_S1]  ctrl
    # G1     [2-1_S2, 2-2_S3, 2-3_S4]    G1
    # G1     [2-1_S2, 2-2_S3, 2-3_S4]    G1
    # G1     [2-1_S2, 2-2_S3, 2-3_S4]    G1
    # G2     [3-1_S5, 3-2_S6, 3-3_S7]    G2
    # G2     [3-1_S5, 3-2_S6, 3-3_S7]    G2
    # G2     [3-1_S5, 3-2_S6, 3-3_S7]    G2
    temp = temp.to_dict()
    # {'sample': {'ctrl': ['1-2_S1'], 'G1': ['2-1_S2', '2-2_S3', '2-3_S4'], 'G2': ['3-1_S5', '3-2_S6', '3-3_S7']}, 
    # 'group': {'ctrl': 'ctrl', 'G1': 'G1', 'G2': 'G2'}}
    group2sample=temp['sample']
    # {'ctrl': ['1-2_S1'], 'G1': ['2-1_S2', '2-2_S3', '2-3_S4'], 'G2': ['3-1_S5', '3-2_S6', '3-3_S7']}

    ### parse contrast
    contrast = pd.read_csv("contrast.csv", comment='#')
    #         type treatment control
    # 0  contrast1        G1    ctrl
    # 1  contrast2        G2    ctrl
    # 2  contrast3     G1;G2    ctrl
    ### contrast2groups
    contrasts =  contrast['type'].tolist()  
    # ['contrast1', 'contrast2', 'contrast3']
    contrastNames = contrast.loc[:,['treatment', 'control']].agg("_vs_".join, axis=1).tolist()
    contrastNames = [x.replace(';', '_') for x in contrastNames] 
    # ['G1_vs_ctrl', 'G2_vs_ctrl', 'G1_G2_vs_ctrl']
    treatmentGroups = [x.split(';') for x in contrast['treatment']]
    # [['G1'], ['G2'], ['G1', 'G2']]
    controlGroups = [x.split(';') for x in contrast['control']]
    # [['ctrl'], ['ctrl'], ['ctrl']
    contrast2contrast_name=dict(zip(contrasts, contrastNames))
    # {'contrast1': 'G1_vs_ctrl', 'contrast2': 'G2_vs_ctrl', 'contrast3': 'G1_G2_vs_ctrl'}
    contrast2treatmentGroups=dict(zip(contrasts, treatmentGroups))
    # {'contrast1': ['G1'], 'contrast2': ['G2'], 'contrast3': ['G1', 'G2']}
    contrast2controlGroups=dict(zip(contrasts, controlGroups))
    # {'contrast1': ['ctrl'], 'contrast2': ['ctrl'], 'contrast3': ['ctrl']}

    ### contrast2groups2samples
    # input: contrast2treatmentGroups, contrast2controlGroups, group2sample
    # output: contrast2treatmentSamples, contrast2controlSamples
    contrast2treatmentSamples = {}
    for c, gs in contrast2treatmentGroups.items():
        # print(c, gs)
        # contrast3 ['G1', 'G2']
        ss = list(map(group2sample.get, gs))
        # [['2-1_S2', '2-2_S3', '2-3_S4'], ['3-1_S5', '3-2_S6', '3-3_S7']]
        ss = reduce(lambda x,y: x+y, ss)
        # ['2-1_S2', '2-2_S3', '2-3_S4', '3-1_S5', '3-2_S6', '3-3_S7']
        contrast2treatmentSamples[c] = ss
        # {'contrast1': ['2-1_S2', '2-2_S3', '2-3_S4'], 
        # 'contrast2': ['3-1_S5', '3-2_S6', '3-3_S7'], 
        # 'contrast3': ['2-1_S2', '2-2_S3', '2-3_S4', '3-1_S5', '3-2_S6', '3-3_S7']}
    contrast2controlSamples = {}
    for c, gs in contrast2controlGroups.items():
        ss = list(map(group2sample.get, gs))
        ss = reduce(lambda x,y: x+y, ss)
        contrast2controlSamples[c] = ss


    return parse_meta_contrast_class(contrast2contrast_name, 
                contrast2treatmentSamples, contrast2controlSamples, contrasts) # dictionaries in class
parse_meta_contrast_obj=parse_meta_contrast(fmeta="meta.csv", fcontrast="contrast.csv") 
# print("parse_meta_contrast_obj:", vars(parse_meta_contrast_obj))
# {'contrast2contrast_name': {'contrast1': 'G1_vs_ctrl', 'contrast2': 'G2_vs_ctrl', 'contrast3': 'G1_G2_vs_ctrl'}, 
# 'contrast2treatmentSamples': {'contrast1': ['2-1_S2', '2-2_S3', '2-3_S4'], 'contrast2': ['3-1_S5', '3-2_S6', '3-3_S7'], 'contrast3': ['2-1_S2', '2-2_S3', '2-3_S4', '3-1_S5', '3-2_S6', '3-3_S7']}, 
# 'contrast2controlSamples': {'contrast1': ['1-2_S1'], 'contrast2': ['1-2_S1'], 'contrast3': ['1-2_S1']}}

def get_treatment_bams_from_contrast(contrast="contrast1", root="DamID_reads/", o = parse_meta_contrast_obj):
    treatment_samples = o.contrast2treatmentSamples[contrast]
    treatment_bams = list(map(lambda x:  root + str(x) + ".bam", treatment_samples))
    # print ("treatment_bams now:", treatment_bams)
    # treatment_bams now: ['DamID_reads/2-1_S2.bam', 'DamID_reads/2-2_S3.bam', 'DamID_reads/2-3_S4.bam', 'DamID_reads/3-1_S5.bam', 'DamID_reads/3-2_S6.bam', 'DamID_reads/3-3_S7.bam']
    #treatment_bams = reduce(lambda x, y: x+" "+y , treatment_bams)
    return treatment_bams
def get_control_bams_from_contrast(contrast="contrast1", root="DamID_reads/", o = parse_meta_contrast_obj):
    control_samples = o.contrast2controlSamples[contrast]
    control_bams = list(map(lambda x:  root + str(x) + ".bam", control_samples))
    return control_bams
def get_contrast_name_from_contrast(contrast="contrast1", o = parse_meta_contrast_obj):
    # print("name", contrast, o.contrast2contrast_name[contrast])
    return o.contrast2contrast_name[contrast]
# rule test_target: # test
#     input:
#         macs2_DamID_contrast = "macs2_DamID_contrast/contrast3/finished"
# todo: end of this part




# load modules (have to use """, to keep in one block)
# - alias does not work, have to use $samstat
shell.prefix("""
            #source /home/rl44w/.bash_profile
            #echo "## snake.prefix"
            HOME=/home/rl44w/
            samtools="singularity exec $HOME/singularity/hand_sandbox.simg samtools"
            samstat="singularity exec $HOME/singularity/hand_sandbox.simg samstat" 
            # alias in .bash_profile not working in snakemake
            bedGraphToBigWig="singularity exec $HOME/singularity/hand_sandbox.simg bedGraphToBigWig"
            """)




rule all:
    input:
        # 1. everything listed here will be produced by the pipeline
        # 2. feed {sample}
        macs2_DamID_sample=expand("macs2_DamID_sample/{sample}_peaks.narrowPeak", sample=SAMPLES),
        macs2_DamID_sample_treat_pileup_bw=expand("macs2_DamID_sample/{sample}_treat_pileup.bw", sample=SAMPLES),
        macs2_DamID_contrast = expand("macs2_DamID_contrast/{contrast}/finished", 
                contrast=parse_meta_contrast_obj.contrasts),
        fastqc="fastqc/multiqc_report.html", # not in main workflow, so list here
        sorted_reads_bam_qc=expand("sorted_reads_bam_qc/stats/{sample}.stats.txt", sample=SAMPLES),
        DamID_reads_bam_qc=expand("DamID_reads_bam_qc/stats/{sample}.stats.txt", sample=SAMPLES),

        qc1="DamID_reads_bam_qc/fingerprint.pdf",
        #qc2="DamID_reads_bam_qc/fragment_size.pdf",
        qc3="DamID_reads_bam_qc/multiBamSummary.heatmap.pdf",
        qc4="DamID_reads_bam_qc/multiBamSummary.pca.pdf",
        qc5=expand("DamID_reads_bam_qc/{sample}.insert_size.pdf", sample=SAMPLES),
        dag="Workflow_DAG.all.svg"






rule fastqc:
    # don't need input, if you agree on not checking them
    # without output, output will not be created
    output:
        "fastqc/multiqc_report.html"
    params:
        mem="1000"
    threads:
        8
    log:
        "log/fastqc/fastqc.log"
    shell:
        # {input/output} don't have to be in command
        # have to load module in one block
        """
        module load fastqc/0.11.5
        mkdir -p fastqc
        mkdir -p fastqc/details
        fastqc -t {threads} fastq/*q.gz -o fastqc/details &> {log}
        multiqc fastqc/details -o fastqc &>> {log}
        """


rule bwa_index:
    input:
        GENOME
    output:
        INDEX
    params:
        mem="8000"
    threads:
        2
    log:
        "log/bwa_index.log"
    shell:
        """
        bwa index -a bwtsw {input} &> {log}
        """


rule bwa_map:
    # 1min/1M reads with 16 cores
    input:
        index=INDEX,
        r1="fastq/{sample}.R1.fastq.gz",
        r2="fastq/{sample}.R2.fastq.gz",
    output:
        temp("mapped_reads/{sample}.bam")
    params:
        mem="1500"  # todo auto adjust based on {threads}, for human need 18G+ 
    conda:
        "envs/samtools.yaml"
    threads:
        16
    log:
        "mapped_reads/{sample}.bam.log"
    benchmark:
        "mapped_reads/{sample}.bam.tsv"
    shell:
        """
        bwa mem -t {threads} {GENOME} \
        {input.r1} {input.r2} \
        2> {log}| samtools view -Sb -1 -@ 2 - -o {output} &>> {log}
        """


rule samtools_sort_index:
    # 2M/min
    input:
        "mapped_reads/{sample}.bam"
    output:
        "sorted_reads/{sample}.bam"
    params:
        mem="1200"
    conda:
        "envs/samtools.yaml"
    threads:
        4
    log:
        "log/samtools_sort/{sample}.sort.log"
    shell:
        """
        samtools --version &> {log}
        samtools sort -@ {threads} -m 1G {input} -o {output} &>> {log}
        samtools index {output} &>> {log}
        """



rule DamID_filter:
    input:
        "sorted_reads/{sample}.bam"
    output:
        "DamID_reads/{sample}.bam"
    log:
        "DamID_reads/{sample}.log"
    benchmark:
        "DamID_reads/{sample}.benchmark.tsv"
    threads:
        1
    params:
        mem="16000"
    shell:
        """
        # need samtools/1.9
        python scripts/filter_bam.py {input} {GENOME} GATC {output} &> {log}
        """


rule sorted_reads_bam_qc:
    input:
        bam="sorted_reads/{sample}.bam"
    output:
        idxstats="sorted_reads_bam_qc/idxstats/{sample}.idxstats.txt",
        flagstat="sorted_reads_bam_qc/flagstat/{sample}.flagsat.txt",
        stats="sorted_reads_bam_qc/stats/{sample}.stats.txt"
    params:
        mem="3000"
    threads:
        4
    log:
        idxstats="sorted_reads_bam_qc/idxstats/{sample}.idxstats.log",
        flagstat="sorted_reads_bam_qc/flagstat/{sample}.flagsat.log",
        stats="sorted_reads_bam_qc/stats/{sample}.stats.log"
    shell:
        """
        samtools idxstats {input.bam} > {output.idxstats} 2> {log.idxstats} &
        samtools flagstat {input.bam} > {output.flagstat} 2> {log.flagstat} &
        samtools stats {input.bam} > {output.stats} 2> {log.stats} &
        wait
        """


rule DamID_reads_bam_qc:
    input:
        bam="DamID_reads/{sample}.bam"
    output:
        idxstats="DamID_reads_bam_qc/idxstats/{sample}.idxstats.txt",
        flagstat="DamID_reads_bam_qc/flagstat/{sample}.flagsat.txt",
        stats="DamID_reads_bam_qc/stats/{sample}.stats.txt"
    params:
        mem="2000"
    threads:
        1
    log:
        idxstats="DamID_reads_bam_qc/idxstats/{sample}.idxstats.log",
        flagstat="DamID_reads_bam_qc/flagstat/{sample}.flagsat.log",
        stats="DamID_reads_bam_qc/stats/{sample}.stats.log"
    shell:
        """
        samtools idxstats {input.bam} > {output.idxstats} 2> {log.idxstats} 
        samtools flagstat {input.bam} > {output.flagstat} 2> {log.flagstat} 
        samtools stats {input.bam} > {output.stats} 2> {log.stats} 
        """


rule plotFingerprint:
    input:
        expand("DamID_reads/{sample}.bam", sample=SAMPLES)
    output:
        plot="DamID_reads_bam_qc/fingerprint.pdf",
        txt="DamID_reads_bam_qc/fingerprint.txt",
    params:
        mem="2000"
    threads:
        6
    log:
        "DamID_reads_bam_qc/fingerprint.log"
    shell:
        """
        plotFingerprint -b {input} \
            --plotFile {output.plot} \
            --outRawCounts {output.txt} \
            --plotTitle "Fingerprint Plot" \
            --smartLabels \
            --minMappingQuality {MQ_MIN} \
            --binSize {BIN_SIZE} \
            --minFragmentLength {config[minFragmentLength]} \
            --maxFragmentLength {config[maxFragmentLength]} \
            --extendReads \
            --centerReads \
            --samFlagInclude 2 \
            -p {threads} &> {log}
        """
        # --samFlagInclude 2: mate properly paired only
        # --extendReads: use mate into


rule bamPEFragmentSize:
    input:
        expand("DamID_reads/{sample}.bam", sample=SAMPLES)
    output:
        plot="DamID_reads_bam_qc/fragment_size.pdf",
        txt="DamID_reads_bam_qc/fragment_size.txt"
    params:
        mem="4000"
    threads:
        4
    log:
        "DamID_reads_bam_qc/fragment_size.log"
    shell:
        """
        bamPEFragmentSize \
        -hist {output.plot} \
        --outRawFragmentLengths {output.txt} \
        -T "Fragment Size Distribution" \
        --maxFragmentLength 2000 \
        -b {input} \
        -p {threads} &> {log}
        """


rule multiBamSummary:
    input:
        expand("DamID_reads/{sample}.bam", sample=SAMPLES)
    output:
        "DamID_reads_bam_qc/multiBamSummary.npz",
    threads:
        8
    params:
        mem="3500"
    log:
        "DamID_reads_bam_qc/multiBamSummary.log"
    shell:
        """
        multiBamSummary bins \
        -b {input} \
        -o {output} \
        --binSize {BIN_SIZE} \
        --smartLabels \
        -p {threads} \
        --minMappingQuality {MQ_MIN} \
        --minFragmentLength {config[minFragmentLength]} \
        --maxFragmentLength {config[maxFragmentLength]} \
        -e \
        --samFlagInclude 2 &> {log}
        """
        

rule plotCorrelation:
    input:
        "DamID_reads_bam_qc/multiBamSummary.npz",
    output:
        "DamID_reads_bam_qc/multiBamSummary.heatmap.pdf"
    threads:
        1
    params:
        mem="20000"
    log:
        "DamID_reads_bam_qc/plotCorrelation.log"
    shell:
        """
        plotCorrelation \
        -in {input} \
        --corMethod pearson --skipZeros \
        --whatToPlot heatmap \
        -T 'Pearson Corr Between Bins' \
        --removeOutliers \
        -o {output} &> {log}
        """

rule plotPCA:
    input:
        "DamID_reads_bam_qc/multiBamSummary.npz",
    output:
        "DamID_reads_bam_qc/multiBamSummary.pca.pdf"
    threads:
        1
    params:
        mem="20000"
    log:
        "DamID_reads_bam_qc/plotPCA.log"
    shell:
        """
        plotPCA \
        --corData {input} \
        --plotFile {output} &> {log}
        """

rule CollectInsertSizeMetrics:
    input:
        "DamID_reads/{sample}.bam"
    output:
        txt="DamID_reads_bam_qc/{sample}.insert_size.txt",
        pdf="DamID_reads_bam_qc/{sample}.insert_size.pdf"
    threads:
        1
    params:
        mem="16000"
    shell:
        """
        module load picard/2.17.8
        PICARD=/share/pkg/picard/2.17.8/picard.jar

        java -Xmx15g -jar $PICARD CollectInsertSizeMetrics \
        I={input} \
        O={output.txt} \
        H={output.pdf}
        """ 

rule macs2_DamID_sample:
    # for each sample
    # todo: contrast
    input:
        "DamID_reads/{sample}.bam"
    output:
        "macs2_DamID_sample/{sample}_peaks.narrowPeak", 
        temp("macs2_DamID_sample/{sample}_treat_pileup.bdg"),
        temp("macs2_DamID_sample/{sample}_control_lambda.bdg")
    threads:
        4
    params:
        mem="8000"
    log:
        "macs2_DamID_sample/{sample}_peaks.narrowPeak.log"
    shell:
        """
        source activate py27  # todo: incorporate to conda env

         macs2 callpeak -t {input} \
         -f BAM --nomodel --shift -60 --extsize 100 -g {GSIZE} -q 0.05 --keep-dup all \
         -n {wildcards.sample} --outdir macs2_DamID_sample -B &> {log}
        """

rule macs2_DamID_sample_treat_pileup_bw:
    input:
        "macs2_DamID_sample/{sample}_treat_pileup.bdg"
    output:
        "macs2_DamID_sample/{sample}_treat_pileup.bw"
    log:
        "macs2_DamID_sample/{sample}_treat_pileup.bw.log"
    threads:
        1
    params:
        mem="16000"
    priority:
        100
    shell:
        """
        sort -k1,1 -k2,2n {input} > macs2_DamID_sample/{wildcards.sample}_treat_pileup.s.bdg 2> {log}
        bedGraphToBigWig="singularity exec $HOME/singularity/hand_sandbox.simg bedGraphToBigWig"
        $bedGraphToBigWig macs2_DamID_sample/{wildcards.sample}_treat_pileup.s.bdg {SizeFile} {output} && \
        rm -f macs2_DamID_sample/{wildcards.sample}_treat_pileup.s.bdg &>> {log}
        """

# rule macs2_site_peak2gtf:
#     input:
#         "macs2_site/{sample}_peaks.narrowPeak"
#     output:
#         "macs2_site/{sample}_peaks.gtf"
#     threads:
#         1
#     params:
#         mem="2000"
#     log:
#         "log/macs2_site/peak2gtf/{sample}.log"
#     shell:
#         """
#         perl scripts/peak2gtf.pl {input} > {output} 
#         """

# rule macs_site_count:
#     input:
#         bam="cleanBam/{sample}.bam",
#         gtf="macs2_site/{sample}_peaks.gtf"
#     output:
#         "macs2_site/{sample}.count.txt"
#     threads:
#         4
#     params:
#         mem="4000"
#     shell:
#         """
#         featureCounts -a {input.gtf} -o {output} \
#         -T {threads} -g gene_id -t peak -s 0 -p -B -C -d 38 -D 2000 \
#         -Q 20 --minOverlap 1 --fracOverlap 0 \
#         {input.bam}
#         """

    

# rule bamCoverage:
#     # for ChIP
#     input:
#         "cleanBam/{sample}.bam"
#     output:
#         "bigWig/{sample}.cpm.bw"
#     threads:
#         8
#     params:
#         mem="1500"  # total 6-10G
#     log:
#         "log/bamCoverage/{sample}.bamCoverage.log"
#     shell:
#         # Aim: same as our downstream filters, extensions
#         """
#         bamCoverage --bam {input} \
#         -o  {output} \
#         --numberOfProcessors {threads} \
#         --outFileFormat bigwig \
#         --normalizeUsing CPM \
#         --minFragmentLength {config[minFragmentLength]} \
#         --maxFragmentLength {config[maxFragmentLength]} \
#         --binSize 10 \
#         -e 150 &> {log}
#         """

rule macs2_DamID_contrast:
    """
    For each contrast

    MACS2: 
    will concatenate all control bam files and treatment bam files anyway, 
    so no need to collapse tech-reps
    """
    input:
        treatment=lambda wildcards: get_treatment_bams_from_contrast(contrast=wildcards.contrast),
        control=lambda wildcards: get_control_bams_from_contrast(contrast=wildcards.contrast),
    output:
        flag=touch("macs2_DamID_contrast/{contrast}/finished"),
        # lambda wildcards: get_contrast_name_from_contrast(contrast=wildcards.contrast)
        # "macs2_DamID_contrast/{contrast}/{contrast_name}_peaks.narrowPeak", 
        # # e.g. "macs2_DamID_contrast/contrast1/G1_vs_ctrl_peaks.narrowPeak"
        # temp("macs2_DamID_contrast/{contrast}/{contrast_name}_treat_pileup.bdg"),
        # temp("macs2_DamID_contrast/{contrast}/{contrast_name}_control_lambda.bdg")
    params:
        contrast_name=lambda wildcards: get_contrast_name_from_contrast(contrast=wildcards.contrast)
    threads:
        4
    params:
        mem="8000"
    log:
        "macs2_DamID_contrast/{contrast}/macs2_DamID.log"
    shell:
        """
        source activate py27  # todo: incorporate to conda env


        macs2 callpeak -t {input.treatment} -c {input.control} \
        -f BAM --nomodel --shift -60 --extsize 100 -g {GSIZE} -q 0.05 --keep-dup all \
        -n {params.contrast_name} --outdir macs2_DamID_contrast -B &> {log}
        """

rule macs2_DamID_contrast_treat_pileup_bw:
    input:
        "macs2_DamID_sample/{sample}_treat_pileup.bdg"
    output:
        "macs2_DamID_sample/{sample}_treat_pileup.bw"
    log:
        "macs2_DamID_sample/{sample}_treat_pileup.bw.log"
    threads:
        1
    params:
        mem="16000"
    priority:
        100
    shell:
        """
        sort -k1,1 -k2,2n {input} > macs2_DamID_sample/{wildcards.sample}_treat_pileup.s.bdg 2> {log}
        bedGraphToBigWig="singularity exec $HOME/singularity/hand_sandbox.simg bedGraphToBigWig"
        $bedGraphToBigWig macs2_DamID_sample/{wildcards.sample}_treat_pileup.s.bdg {SizeFile} {output} && \
        rm -f macs2_DamID_sample/{wildcards.sample}_treat_pileup.s.bdg &>> {log}
        """

rule create_dag:
    params:
        mem="1000"  
        # every job has to have this defined 
        # to use snakemake --cluster 'bsub -q short -R "rusage[mem={params.mem}]" -n {threads}'
    threads:
        1
    output:
        "Workflow_DAG.all.svg"
    log:
        "log/create_dag/Workflow_DAG.all.svg.log"
    shell:
        "snakemake --dag all | dot -Tsvg > {output} 2> {log}"


rule reset:
    shell:
        """
        rm -rf fastqc 
        snakemake --unlock
        """

